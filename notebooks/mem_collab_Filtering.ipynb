{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350193e3",
   "metadata": {},
   "source": [
    "# <u> Memory-based Collaborative Filtering</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30aaa",
   "metadata": {},
   "source": [
    "This notebook explores **memory-based collaborative filtering** as a first baseline for building a movie recommendation system. The approach relies solely on the raw user–item rating matrix, without the need for training or any machine learning framework.\n",
    "\n",
    "The general idea is to build recommendations based on similarities, which can be computed in two main ways:\n",
    "\n",
    "- **User–User Collaborative Filtering**: recommends movies liked by users who have similar preferences and rating behavior to us.\n",
    "- **Item–Item Collaborative Filtering**: recommends new movies that are similar to the ones we have already seen and liked.\n",
    "\n",
    "<br>\n",
    "\n",
    "To define similarities between users or items, we commonly use two metrics:\n",
    "\n",
    "- **Cosine Similarity**: a fast and popular measure that computes the angle between two vectors (single users or items). The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{cosine}}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Pearson Correlation**: more computationally demanding, but it measures the linear relationship between co-rated items, correcting for each user’s individual rating bias. The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{pearson}}(x, y) = \\frac{\\sum_{i}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2} \\sqrt{\\sum(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Due to its ability to correct for individual rating biases, Pearson correlation is particularly effective in user–user collaborative filtering, where users may differ significantly in how they rate items. However, in item–item collaborative filtering, this adjustment is not appropriate, as centering on item means does not address user-level biases. Instead, cosine similarity and its adjusted form,are used to account for user rating behavior when comparing items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c809320",
   "metadata": {},
   "source": [
    "## <u>0. Setting:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cabae",
   "metadata": {},
   "source": [
    "### <u>0.1 Import libraries and dataframe</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd, numpy as np, os, sys, seaborn as sns, matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from surprise import Dataset, Reader, KNNWithMeans, accuracy\n",
    "import time\n",
    "\n",
    "# Set the working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import module for data processing\n",
    "from modules.data_analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d6e11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_bayes_avg</th>\n",
       "      <th>log_count_review</th>\n",
       "      <th>release_year</th>\n",
       "      <th>user_avg_rating</th>\n",
       "      <th>user_avg_bayes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "      <td>3.212004</td>\n",
       "      <td>10.009828</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "      <td>3.950567</td>\n",
       "      <td>9.050289</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "      <td>3.897763</td>\n",
       "      <td>10.713995</td>\n",
       "      <td>1995</td>\n",
       "      <td>3.742857</td>\n",
       "      <td>3.630537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating           timestamp  movie_bayes_avg  \\\n",
       "0       1        2     3.5 2005-04-02 23:53:47         3.212004   \n",
       "1       1       29     3.5 2005-04-02 23:31:16         3.950567   \n",
       "2       1       32     3.5 2005-04-02 23:33:39         3.897763   \n",
       "\n",
       "   log_count_review  release_year  user_avg_rating  user_avg_bayes  \n",
       "0         10.009828          1995         3.742857        3.630537  \n",
       "1          9.050289          1995         3.742857        3.630537  \n",
       "2         10.713995          1995         3.742857        3.630537  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import cleand dataframe\n",
    "file_path = '../data/processed/ratings_processed.parquet'\n",
    "ratings_processed = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "ratings_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df43ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique userId: 138383\n",
      "Number of unique movieId: 12531\n",
      "Number of reviews: ~ 19.9 M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique userId: {ratings_processed['userId'].nunique()}\")\n",
    "print(f\"Number of unique movieId: {ratings_processed['movieId'].nunique()}\")\n",
    "print(f\"Number of reviews: ~ {len(ratings_processed)/1000000:.1f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d000ea",
   "metadata": {},
   "source": [
    "### <u>0.2 Model evaluation</u>\n",
    "\n",
    "To evaluate model performance and enable fair comparison with future approaches, we adopt a **train-validation-test split**: for each user, the last 4 ratings are held out for testing, while the 5th and 6th most recent ratings are used for validation. The remaining ratings are used for training the model. This setup mirrors real-world scenarios where unseen items are recommended based on past behavior, while the validation set is used for hyperparameter tuning to improve model generalization.\n",
    "\n",
    "Model accuracy is assessed using the **Root Mean Squared Error (RMSE)** between predicted and actual ratings on the held-out items. Taking the square root of the Mean Squared Error ensures that the metric is expressed on the same scale as the original ratings, providing an interpretable measure of prediction error.\n",
    "\n",
    "Holding out the 2–4 most recent ratings per user provides a good balance between training size and evaluation coverage across the entire dataset. Since we only include users with at least 20 ratings, this results in a minimum of approximately **70% train, 10% validation, and 20% test split per user**. Although the percentage of held-out ratings per user is small, the large size of the dataset ensures a substantial number of data points, enabling robust and reliable validation and evaluation across the user base.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cf1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in term of review date and user id and then pick 4 most recent review for each userId to build the test set \n",
    "sorted_df = ratings_processed.sort_values(by=['userId','timestamp'], ascending=[True,True])\n",
    "test_df = sorted_df.groupby('userId').tail(4)\n",
    "\n",
    "# Pick the 5th and 6th most recent reviews for each userId to build the validation set\n",
    "val_df = sorted_df.groupby('userId').tail(6).groupby('userId').head(2)\n",
    "\n",
    "# Build train df by removing the test_df and val_df rows from it\n",
    "train_df = ratings_processed.drop(test_df.index).drop(val_df.index).reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Select only necessary columns\n",
    "train_df = train_df[['userId', 'movieId', 'rating']]\n",
    "test_df = test_df[['userId', 'movieId', 'rating']]\n",
    "val_df = val_df[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9309d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert correct length across users in the validation and test set\n",
    "assert test_df.groupby('userId').size().eq(4).all(), \"Some users in test_df have ≠ 4 ratings\"\n",
    "assert val_df.groupby('userId').size().eq(2).all(), \"Some users in val_df have ≠ 2 ratings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38ac292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserId 1 - Train set length: 169\n",
      "UserId 1 - Validation set length: 2\n",
      "UserId 1 - Test set length: 4\n",
      "\n",
      "Train set sample for userId = 1:\n",
      "   userId  movieId  rating\n",
      "0       1        2     3.5\n",
      "1       1       29     3.5\n",
      "2       1       32     3.5\n",
      "\n",
      "Validation set sample for userId = 1:\n",
      "   userId  movieId  rating\n",
      "0       1     1525     3.0\n",
      "1       1     5999     3.5\n",
      "\n",
      "Test set sample for userId = 1:\n",
      "   userId  movieId  rating\n",
      "0       1     7449     3.5\n",
      "1       1     4133     3.0\n",
      "2       1     3997     3.5\n",
      "3       1     1750     3.5\n"
     ]
    }
   ],
   "source": [
    "# Find length of train, validation, and test set for a given userId\n",
    "user_id = 1\n",
    "user_1_train_len = train_df[train_df['userId'] == user_id].shape[0]\n",
    "user_1_val_len = val_df[val_df['userId'] == user_id].shape[0]\n",
    "user_1_test_len = test_df[test_df['userId'] == user_id].shape[0]\n",
    "print(f\"UserId {user_id} - Train set length: {user_1_train_len}\")\n",
    "print(f\"UserId {user_id} - Validation set length: {user_1_val_len}\")\n",
    "print(f\"UserId {user_id} - Test set length: {user_1_test_len}\")\n",
    "\n",
    "# print sample of train, validation, and test set for userId \n",
    "print(f\"\\nTrain set sample for userId = {user_id}:\")\n",
    "print(train_df[train_df['userId'] == user_id].head(3))\n",
    "print(f\"\\nValidation set sample for userId = {user_id}:\")\n",
    "print(val_df[val_df['userId'] == user_id].head(2))\n",
    "print(f\"\\nTest set sample for userId = {user_id}:\")\n",
    "print(test_df[test_df['userId'] == user_id].head(4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468e7e7",
   "metadata": {},
   "source": [
    "## <u> 1. User-User Collaborative Filtering </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9691b",
   "metadata": {},
   "source": [
    "As previously explained, the main idea behind the **user–user collaborative filtering** approach is to recommend items to a target user by finding other users with similar tastes, then suggesting items they liked but the target user hasn't seen yet.\n",
    "\n",
    "To compute similarities across users, we use Pearson **correlation** on the **user–item rating matrix**, which is structured as follows:\n",
    "- Rows represent users (`userId`)\n",
    "- Columns represent movies (`movieId`)\n",
    "- Values are the ratings users assigned to each movie\n",
    "\n",
    "This results in a very sparse matrix, as the dataset contains approximately 139,000 users and 12,000 movies, and most users rate only a small fraction of all available movies.\n",
    "\n",
    "The prediction for how much a user $u$ will like an unseen item $i$ is computed using a **similarity-weighted average** of the ratings from the most similar users who have rated item $i$:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u,v)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$\n",
    "- $\\bar{r}_u$ is the average rating of user $u$\n",
    "- $N(u)$ is the set of top-$K$ most similar users to $u$ who rated item $i$\n",
    "- $\\text{sim}(u,v)$ is the Pearson correlation between users $u$ and $v$\n",
    "- $r_{v,i}$ is the rating that user $v$ gave to item $i$\n",
    "- $\\bar{r}_v$ is the average rating of user $v$\n",
    "\n",
    "<br>\n",
    "\n",
    "For the purpose of **hyperparameter tuning**, we evaluate different values of $K$, meaning that predictions are based on the top-$K$ most similar users who have rated each item. The optimal value of $K$ balances the trade-off between smoothing out noise from individual ratings and focusing on the most relevant users. Additionally, by setting a **minimum of 25 reviews per movie**, we ensure that each item has sufficient rating data to support reliable top-$K$ neighbor selection during prediction.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee7a5e",
   "metadata": {},
   "source": [
    "### <u> 1.1 Data Preparation for surprise</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dfe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rating scale (0.5-5)\n",
    "reader = Reader(rating_scale=(0.5,5))\n",
    "\n",
    "# Load training set into Surprise dataset\n",
    "trainset = Dataset.load_from_df(train_df, reader).build_full_trainset()\n",
    "\n",
    "# Prepare validation and test sets as lists of tuples\n",
    "valset= [(row.userId, row.movieId, row.rating) for row in val_df.itertuples(index=False)]\n",
    "testset = [(row.userId, row.movieId, row.rating) for row in test_df.itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f96529",
   "metadata": {},
   "source": [
    "### <u> 1.2 Hyperparameter tuning </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da213b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Loop over different k values\n",
    "for k in [5,10,20,30,40]:\n",
    "    model = KNNWithMeans(k=k, sim_options = {\"name\":\"pearson\", \"user_based\":True})\n",
    "    model.fit(trainset)\n",
    "    pred_val = model.test(valset)\n",
    "    rmse = accuracy.rmse(pred_val,verbose = False)\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_k = k\n",
    "\n",
    "# Print best hyperparameter along with the respective validation loss\n",
    "print('Hyperparameter tuning for user-user collaborative filtering:')\n",
    "print(f\"Best k: {best_k} with validation RMSE:{best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23c2e8",
   "metadata": {},
   "source": [
    "### <u> 1.3 Model Evaluation </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Train final model with the best hyperparemeter\n",
    "model = KNNWithMeans(k=best_k, sim_options = {\"name\":\"pearson\", \"user_based\":True})\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate final model on test set\n",
    "pred_test = model.test(testset)\n",
    "rmse_test_u_u = accuracy.rmse(pred_test, verbose = False)\n",
    "mae_test_u_u = accuracy.mae(pred_test, verbose = False)\n",
    "\n",
    "# Training and Evaluation time\n",
    "elapsed_time_u_u = time.perf_counter() - start\n",
    "\n",
    "# Summary model's performance\n",
    "print(f\"Model performance user-user collaborative filtering:\\n RMSE: {rmse_test_u_u}\\n MAE: {mae_test_u_u}\\n Time: {elapsed_time_u_u} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b497e4",
   "metadata": {},
   "source": [
    "## <u> 2. Item–Item Collaborative Filtering </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164794a2",
   "metadata": {},
   "source": [
    "As an alternative to user–user collaborative filtering, item–item collaborative filtering focuses on relationships between items rather than users. Instead of identifying users with similar preferences, recommendations are generated by comparing items previously rated by a user with the target item. This approach is generally more stable and computationally efficient, since items tend to receive more ratings than individual users, resulting in more reliable similarity estimates.\n",
    "\n",
    "<br>\n",
    "\n",
    "**<u>Similarity:</u>**\n",
    "\n",
    "Item similarity is computed using **adjusted cosine similarity**, which corrects for user rating biases by centering each rating around the corresponding user’s average. This prevents misleading similarity scores caused by users who consistently rate higher or lower than others. For two items $i$ and $j$, adjusted cosine similarity is defined as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}(i, j)\n",
    "=\n",
    "\\frac{\\sum_{u \\in U_{ij}} (R_{ui} - \\bar{R}_u) (R_{uj} - \\bar{R}_u)}\n",
    "{\\sqrt{\\sum_{u \\in U_{ij}} (R_{ui} - \\bar{R}_u)^2} \\cdot \n",
    " \\sqrt{\\sum_{u \\in U_{ij}} (R_{uj} - \\bar{R}_u)^2}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where:  \n",
    "- $U_{ij}$ denotes the set of users who have rated both items $i$ and $j$  \n",
    "- $R_{ui}$ is the rating given by user $u$ to item $i$  \n",
    "- $R_{uj}$ is the rating given by user $u$ to item $j$  \n",
    "- $\\bar{R}_u$ is the average rating given by user $u$, which removes user-specific bias  \n",
    "\n",
    "<br>\n",
    "\n",
    "**<u>Prediction:</u>**\n",
    "\n",
    "A predicted rating $\\hat{R}_{ui}$ for user $u$ on item $i$ is computed as a weighted average of the user’s deviations from the mean on items similar to $i$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\hat{R}_{ui}\n",
    "=\n",
    "\\bar{R}_i +\n",
    "\\frac{\\sum_{j \\in N(i;u)} \\text{sim}(i, j) \\cdot (R_{uj} - \\bar{R}_j)}\n",
    "{\\sum_{j \\in N(i;u)} |\\text{sim}(i, j)|}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where:  \n",
    "- $\\bar{R}_i$ is the average rating of item $i$ across all users  \n",
    "- $N(i;u)$ denotes the set of the top $k$ most similar items to item $i$ that have been rated by user $u$  \n",
    "- $\\bar{R}_j$ is the average rating of item $j$  \n",
    "- $\\text{sim}(i,j)$ is the adjusted cosine similarity between items $i$ and $j$  \n",
    "- $R_{uj}$ is the rating of user $u$ for item $j$  \n",
    "\n",
    "<br>\n",
    "\n",
    "To ensure stable and accurate predictions, the neighborhood size $k$ is **treated as a hyperparameter** and evaluated on the validation set. This allows the model to find an optimal trade-off between smoothing noise from individual ratings and focusing on the most relevant items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb0d73",
   "metadata": {},
   "source": [
    "### <u> 2.1 Data Preparation for surprise</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd903718",
   "metadata": {},
   "source": [
    "Given that the Surprise library does not natively implement adjusted cosine similarity, a correction for each user’s average rating is applied to the ratings before computing standard cosine similarity. This approach effectively reproduces the adjusted cosine similarity while maintaining the computational efficiency provided by the Surprise package. To avoid data leakage, the user means are computed on the training set only and then applied to the validation and test sets during prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean_rating over train dataframe\n",
    "user_means = train_df.groupby('userId')['rating'].mean()\n",
    "\n",
    "# Centralise dataframes (train, validation, and test sets)\n",
    "train_df_centered = train_df.copy()\n",
    "train_df_centered['rating'] = train_df_centered.apply(lambda row: row['rating'] - user_means[row['userId']], axis=1)\n",
    "\n",
    "val_df_centered = val_df.copy()\n",
    "val_df_centered['rating'] = val_df_centered.apply(lambda row: row['rating'] - user_means[row['userId']], axis=1)\n",
    "\n",
    "test_df_centered = test_df.copy()\n",
    "test_df_centered['rating'] = test_df_centered.apply(lambda row: row['rating'] - user_means[row['userId']], axis=1)\n",
    "\n",
    "# Define centered rating scale\n",
    "min_rating = train_df_centered['rating'].min()\n",
    "max_rating = train_df_centered['rating'].max()\n",
    "reader = Reader(rating_scale=(min_rating,max_rating))\n",
    "\n",
    "# Load training set into Surprise dataset\n",
    "trainset = Dataset.load_from_df(train_df_centered, reader).build_full_trainset()\n",
    "\n",
    "# Prepare validation and test sets as lists of tuples\n",
    "valset = [(row.userId, row.movieId, row.rating) for row in val_df_centered.itertuples(index=False)]\n",
    "testset = [(row.userId, row.movieId, row.rating) for row in test_df_centered.itertuples(index=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e28a42",
   "metadata": {},
   "source": [
    "### <u> 2.2 Hyperparameter tuning </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f520dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Loop over different k values\n",
    "for k in [5,10,20,30,40]:\n",
    "    model = KNNWithMeans(k=k, sim_options = {\"name\":\"cosine\", \"user_based\":False})\n",
    "    model.fit(trainset)\n",
    "    pred_val = model.test(valset)\n",
    "    rmse = accuracy.rmse(pred_val,verbose = False)\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_k = k\n",
    "\n",
    "# Print best hyperparameter along with the respective validation loss\n",
    "print('Hyperparameter tuning for item-item collaborative filtering:')\n",
    "print(f\"Best k: {best_k} with validation RMSE:{best_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d41b1",
   "metadata": {},
   "source": [
    "### <u> 2.3 Model Evaluation </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e86f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Train final model with the best hyperparemeter\n",
    "model = KNNWithMeans(k=best_k, sim_options = {\"name\":\"cosine\", \"user_based\":True})\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate final model on test set\n",
    "pred_test = model.test(testset)\n",
    "rmse_test_i_i = accuracy.rmse(pred_test, verbose = False)\n",
    "mae_test_i_i = accuracy.mae(pred_test, verbose = False)\n",
    "\n",
    "# Training and Evaluation time\n",
    "elapsed_time_i_i = time.perf_counter() - start\n",
    "\n",
    "# Summary model's performance\n",
    "print(f\"Model performance item-item collaborative filtering: \\n RMSE: {rmse_test_i_i}\\n MAE: {mae_test_i_i}\\n Time: {elapsed_time_i_i} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f512a9",
   "metadata": {},
   "source": [
    "## <u> 3. Summary of findings </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac05f2",
   "metadata": {},
   "source": [
    "### <u> 3.1 Model performance </u>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surprise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
