{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350193e3",
   "metadata": {},
   "source": [
    "# <u> Memory-based Collaborative Filtering</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30aaa",
   "metadata": {},
   "source": [
    "This notebook explores **memory-based collaborative filtering** as a first baseline for building a movie recommendation system. The approach relies solely on the raw user–item rating matrix, without the need for training or any machine learning framework.\n",
    "\n",
    "The general idea is to build recommendations based on similarities, which can be computed in two main ways:\n",
    "\n",
    "- **User–User Collaborative Filtering**: recommends movies liked by users who have similar preferences and rating behavior to us.\n",
    "- **Item–Item Collaborative Filtering**: recommends new movies that are similar to the ones we have already seen and liked.\n",
    "\n",
    "<br>\n",
    "\n",
    "To define similarities between users or items, we commonly use two metrics:\n",
    "\n",
    "- **Cosine Similarity**: a fast and popular measure that computes the angle between two vectors (single users or items). The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{cosine}}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Pearson Correlation**: more computationally demanding, but it measures the linear relationship between co-rated items, correcting for each user’s individual rating bias. The formula is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\text{pearson}}(x, y) = \\frac{\\sum_{i}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2} \\sqrt{\\sum(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Due to its ability to correct for individual rating biases, Pearson correlation is particularly effective in user–user collaborative filtering, where users may differ significantly in how they rate items. However, in item–item collaborative filtering, this adjustment is not appropriate, as centering on item means does not address user-level biases. Instead, cosine similarity and its adjusted form,are used to account for user rating behavior when comparing items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c809320",
   "metadata": {},
   "source": [
    "## <u>0. Setting:</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cabae",
   "metadata": {},
   "source": [
    "### <u>0.1 Import libraries and dataframe</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd, numpy as np, os, sys, seaborn as sns, matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Set the working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import module for data processing\n",
    "from modules.data_analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d6e11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import cleand dataframe\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33m../data/processed/ratings_processed.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ratings_processed = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m ratings_processed.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\site-packages\\pandas\\io\\parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\site-packages\\pandas\\io\\parquet.py:79\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\site-packages\\pandas\\io\\parquet.py:164\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow is required for parquet support.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yuric\\anaconda3\\envs\\surprise_env\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# Import cleand dataframe\n",
    "file_path = '../data/processed/ratings_processed.parquet'\n",
    "ratings_processed = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "ratings_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df43ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique userId: 138383\n",
      "Number of unique movieId: 12531\n",
      "Number of reviews: ~ 19.9 M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique userId: {ratings_processed['userId'].nunique()}\")\n",
    "print(f\"Number of unique movieId: {ratings_processed['movieId'].nunique()}\")\n",
    "print(f\"Number of reviews: ~ {len(ratings_processed)/1000000:.1f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d000ea",
   "metadata": {},
   "source": [
    "### <u>0.2 Model evaluation </U>\n",
    "In order to evaluate model performance and enable fair comparison with future approaches, we adopt a Leave-5-out strategy: for each user, the last 5 ratings are held out for testing, while the rest are used for training. This mirrors real-world scenarios where unseen items are recommended based on past behavior.\n",
    "We assess accuracy using Root Mean Squared Error (RMSE) between predicted and actual ratings on the held-out items. This evaluation setup provides a consistent and realistic benchmark across all models.\n",
    "\n",
    "The choice of holding out 5 ratings per user ensures a good balance between training size and evaluation coverage. Since we only include users with at least 20 ratings, this results in a minimum 75% train and 25% test split per user. Based on the user activity distribution observed in the `eda.fe.ipynb` notebook, this threshold offers sufficient signal for training while ensuring each user contributes to model evaluation.\n",
    "\n",
    "Additionally with approximately 138,000 unique users in the final dataset, this strategy yields around 690,000 test points for evaluation, out of nearly 20 million total ratings, providing robust and reliable validation across the user base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4cf1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in term of review date and user id and then pick 5 most recent review for each userId\n",
    "sorted_df = ratings_processed.sort_values(by=['userId','timestamp'], ascending=[True,True])\n",
    "test_df = sorted_df.groupby('userId').tail(5)\n",
    "\n",
    "# Build train df by removing the test_df rows from it\n",
    "train_df = ratings_processed.drop(test_df.index).reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9309d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert correct length across users in the test set\n",
    "assert test_df.groupby('userId').size().eq(5).all(), \"Some users in test_df have ≠ 5 ratings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Check of test_train split\n",
    "user_id = 1\n",
    "\n",
    "# All ratings for that user, sorted as in the previous setting\n",
    "user_ratings = ratings_processed[ratings_processed['userId'] == user_id].sort_values('timestamp', ascending=True)\n",
    "\n",
    "# Check that the 5 most recent ratings match test_df's entries for that user\n",
    "expected_test_rows = user_ratings.tail(5).reset_index(drop=True)\n",
    "actual_test_rows = test_df[test_df['userId'] == user_id].reset_index(drop=True)\n",
    "\n",
    "# Assert that they match\n",
    "assert expected_test_rows[['movieId', 'timestamp']].equals(actual_test_rows[['movieId', 'timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468e7e7",
   "metadata": {},
   "source": [
    "## <u> 1. User-User Collaborative Filtering </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9691b",
   "metadata": {},
   "source": [
    "As previously explained, the main idea behind the **user–user collaborative filtering** approach is to recommend items to a target user by finding other users with similar tastes, then suggesting items they liked but the target user hasn't seen yet.\n",
    "\n",
    "To compute similarities across users, we use Pearson **correlation** on the **user–item rating matrix**, which is structured as follows:\n",
    "- Rows represent users (`userId`)\n",
    "- Columns represent movies (`movieId`)\n",
    "- Values are the ratings users assigned to each movie\n",
    "\n",
    "This results in a very sparse matrix, as the dataset contains approximately 139,000 users and 12,000 movies, and most users rate only a small fraction of all available movies.\n",
    "\n",
    "The prediction for how much a user $u$ will like an unseen item $i$ is computed using a **similarity-weighted average** of the ratings from the most similar users who have rated item $i$:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u,v)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{r}_{u,i}$ is the predicted rating for user $u$ on item $i$\n",
    "- $\\bar{r}_u$ is the average rating of user $u$\n",
    "- $N(u)$ is the set of top-$K$ most similar users to $u$ who rated item $i$\n",
    "- $\\text{sim}(u,v)$ is the Pearson correlation between users $u$ and $v$\n",
    "- $r_{v,i}$ is the rating that user $v$ gave to item $i$\n",
    "- $\\bar{r}_v$ is the average rating of user $v$\n",
    "\n",
    "<br>\n",
    "\n",
    "For the sake of efficiency and prediction stability, we set $K=20$, meaning that predictions are based on the 20 most similar users who have rated the item. This value offers a good trade-off: it is large enough to smooth out noise from individual ratings, while still focusing on the most relevant users.\n",
    "Additionally, by setting the minimum number of reviews per movie to 25, we ensure that each item has enough rating data to support consistent top-K neighbor selection during prediction.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c42dfe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaMemoryError: The conda process ran out of memory. Increase system memory and/or try again.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): failed\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a603fa1",
   "metadata": {},
   "source": [
    "## <u> 2. Item-Item Collaborative Filtering </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b497e4",
   "metadata": {},
   "source": [
    "As an alternative to user–user collaborative filtering, which is often inefficient due to high user sparsity and unstable similarity estimates, **item–item collaborative filtering** shifts the perspective to the items themselves. Instead of finding similar users, we recommend items to a user based on the similarity between items they have already rated and the target item. This method is typically more computationally efficient and stable, since items (e.g., movies) receive many ratings and exhibit more consistent co-rating patterns across users. As in the previous approach, we use a sparse rating matrix to represent user-item interactions, but this time structured as an **item–user matrix**, where each row corresponds to a movie and each column to a user. While the matrix is still sparse overall, the item dimension tends to be denser and more stable than the user dimension, making similarity computation more reliable and less expensive.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**<u>Similiraty:</u>**\n",
    "\n",
    "To compute item–item similarities, we use **adjusted cosine similarity**, which corrects for user rating biases by centering each rating around the corresponding user’s average. This prevents misleading similarity scores caused by users who consistently rate much higher or lower than others.\n",
    "\n",
    "Let $R_{ui}$ denote the rating of user $u$ for item $i$, and let $\\bar{R}_u$ denote the average rating by user $u$. The adjusted cosine similarity between items $i$ and $j$ is defined as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{sim}(i, j) = \\frac{\\sum_{u \\in U_{ij}} (R_{ui} - \\bar{R}_u)(R_{uj} - \\bar{R}_u)}{\\sqrt{\\sum_{u \\in U_{ij}} (R_{ui} - \\bar{R}_u)^2} \\cdot \\sqrt{\\sum_{u \\in U_{ij}} (R_{uj} - \\bar{R}_u)^2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $U_{ij}$ is the set of users who rated both items $i$ and $j$\n",
    "- The user mean $\\bar{R}_u$ removes user-specific bias\n",
    "\n",
    "This differs from standard cosine similarity, which does not correct for user bias, and from Pearson correlation, which centers on item means, making it less suitable for item–item comparison.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<u> **Prediction:** </u>\n",
    "\n",
    "The prediction formulation are based on the assumption that a user’s opinion on an item can be inferred from how they rated similar items, adjusted for each item's overall average reception across the user base. Let $\\hat{R}_{ui}$ be the predicted rating for user $u$ on item $i$. The prediction is computed using the baseline average rating of item $i$ and a weighted sum of the user's deviations from the mean on similar items:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\hat{R}_{ui} = \\bar{R}_i + \\frac{\\sum_{j \\in N(i;u)} \\text{sim}(i, j) \\cdot (R_{uj} - \\bar{R}_j)}{\\sum_{j \\in N(i;u)} |\\text{sim}(i, j)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{R}_i$ is the average rating of item $i$ across all users\n",
    "- $N(i;u)$ is the set of the top $k$ most similar items to $i$ that user $u$ has rated\n",
    "- $\\bar{R}_j$ is the average rating of item $j$\n",
    "- $R_{uj}$ is the rating of user $u$ for item $j$\n",
    "\n",
    "<br>\n",
    "\n",
    "To ensure stable and computationally efficient predictions, we limit the similarity neighborhood size $k$ to 10. Additionally, only positively similar items are considered to avoid misleading predictions when the user’s rated movies are unrelated to the target item. If fewer than $k$ similar items are available, the prediction still proceeds using the available subset, falling back more heavily on the global average rating $\\bar{R}_i$ of the target item. This fallback provides a robust baseline while still allowing for personalization when enough neighbors are available.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>**Extension:**</u>\n",
    "\n",
    "One important extension to this method involves incorporating content-based similarity alongside rating-based similarity. In particular, item metadata such as genre can be encoded using multi-hot vectors and used to compute genre-based cosine similarity. This additional information can improve recommendation quality, especially in sparse or cold-start scenarios where rating-based item similarity is limited or unavailable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb0d73",
   "metadata": {},
   "source": [
    "### <u>Preparation</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4294a6e",
   "metadata": {},
   "source": [
    "#### <u> 2.1.1 Build item-user rating matrix:</u> \n",
    "\n",
    "To save memory and avoid redundant computation, we reuse the sparse matrix constructed in section `1.1.1` for user–user collaborative filtering (`item_user_sparse`). As a reminder, it was built using csr_matrix, and the corresponding mappings between original and encoded userID and item are already available in the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surprise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
